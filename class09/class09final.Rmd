---
title: "class09"
author: "Meisha Khan"
date: "2/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering 

Let's try the `kmeans()` function in R to cluster some made-up example data. 

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```



Use the kmeans() function setting k to 2 and nstart=20
## KM <- kmeans(x, centers =2, nstart= 20)
```{r}
km <- kmeans(x, centers= 2, nstart= 20)
```


Inspect/print the results
```{r}
print(km)
```

what is in the output object `km`? I can use `attributes()` function to to find this info :-)

```{r}
attributes(km) 
```


Q. How many points are in each cluster?
```{r}
km$size
```


Q. What ‘component’ of your result object details

 - cluster size?
```{r}
km$withinss
```
 
 - cluster assignment/membership?
```{r}
km$cluster
```
 
 - cluster center?
```{r}
km$centers
```
 
 let's check how many 2s and 1s are in this vector with the `table()` function. 
```{r}
table(km$cluster)
```
 
 
Plot x colored by the kmeans cluster assignment and
 add cluster centers as blue points

```{r}
#plot(x, col = km$cluster, col = km$centers)
```

```{r}
#c(rep("red", 30), rep("blue, 30))
plot(x, col=km$cluster)
```



```{r}
#c(rep("red", 30), rep("blue, 30))
plot(x, col=km$cluster+2)
```


```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=3)
```


## H-clustering

# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)

# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)

# the print method is not so useful here
hc

Call:

hclust(d = dist_matrix)
hc <- hclust(dist(x))

---------------------------

The `hclust()` function is the main Hierarchical clustering method in R and it **must** be passed a *distance matrix* as input, not your raw data!

```{r}
hc <- hclust(dist(x))
hc
```

```{r}
plot(hc)
abline(h=6, col="red", lty=2)
abline(h=3.5, col="blue", lty=2)
```

#draws a dendrogram
```{r}
cutree(hc, h=6) # cut by height h
```

```{r}
cutree(hc, h=3.5) 
```

```{r}
table(cutree(hc, h=3.5))
```

you can also ask `cutree()` for the `k` number of groups that you want. 
```{r}
cutree(hc, k=5)
```


```{r}
#x.grp1 <- 
```


# Step 1. Generate some example data for clustering
```{r}
x <- rbind(
 matrix(rnorm(100, mean=0, sd=0.3), ncol = 2), # c1
 matrix(rnorm(100, mean=1, sd=0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean=1, sd=0.3), # c3
 rnorm(50, mean=0, sd=0.3)), ncol = 2))
colnames(x) <- c("x", "y")
```


# Step 2. Plot the data without clustering
```{r}
plot(x)
```


# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
```{r}
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
```{r}
hc <- hclust(dist(x))
plot(hc)

#plot(cutree(hc, k=3))

grps3 <- cutree(hc, k=3)
grps3
```

```{r}
table(grps3)
```


```{r}
plot(x, col=grps3)
```
 
 
 
Q. How does this compare to your known 'col' groups?

```{r}
table(grps3, col)
```

## Principal Component Analysis (PCA)
## UK FOOODS

the main function in base R for PCA is called `promp()`. Here we will use PCA to examine the funny food that foalks eat in the UK and N. Ireland. 

Import the CSV file first:

```{r}
x <- read.csv("UK_foods.csv", row.names = 1)
x
```

Make some conventional plots
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```


```{r}
pairs(x, col=rainbow(10), pch=16)
```

# PCA to the rescue!

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```


```{r}
attributes(pca)
```


```{r}
plot( pca$x[,1], pca$x[,2],xlab="PC1 67%", ylab="PC2 29%") 
text(pca$x[,1], pca$x[,2], labels = colnames(x), col=c("black", "red", "blue", "darkgreen"))
     
     
```


```{r}
plot(pca)
```

















